{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert from scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNeOfz32O5MKF8Vq7W9XbFt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xelav/transformers-from-scratch/blob/master/Bert_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L6Uy04sxiAQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.src_embed = None\n",
        "        self.tgt_embed = None\n",
        "        self.generator = None\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "\n",
        "        x = self.encoder(self.src_embed(src), src_mask)\n",
        "        x = self.decoder(x, src_mask, tgt, tgt_mask)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_size, eps=1e-8):\n",
        "\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.layer_size = layer_size\n",
        "        self.alpha = nn.Parameter(torch.ones(self.layer_size))\n",
        "        self.beta = nn.Parameter(torch.zeros(self.layer_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x size: (batch_size, seq_len, d_model)\n",
        "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
        "        x_tilde = self.alpha*x_hat + self.beta\n",
        "        return x_tilde\n",
        "\n",
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "\n",
        "    d_k = q.size(-1)\n",
        "    assert q.size(-1) == k.size(-1)\n",
        "\n",
        "    scores = q @ k.transpose(-2,-1)\n",
        "    scores /= np.sqrt(k.size(-1))\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "\n",
        "    scores = scores @ v\n",
        "\n",
        "    return scores\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.v_linear = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
        "        self.k_linear = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
        "        self.q_linear = [nn.Linear(d_model, d_k) for _ in range(h)]\n",
        "\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.output_linear = nn.Linear(d_k * h, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, v, k, q, mask=None):\n",
        "\n",
        "        attention_out_list = []\n",
        "        for i in range(self.h):\n",
        "            v2 = self.v_linear[i](v)\n",
        "            k2 = self.k_linear[i](k)\n",
        "            q2 = self.q_linear[i](q)\n",
        "\n",
        "            attention_out = self.attention(v2, k2, q2, mask, self.dropout)\n",
        "            attention_out_list.append(attention_out)\n",
        "\n",
        "        attention_out_list = torch.cat(attention_out_list, dim=-1)\n",
        "        return self.output_linear(attention_out_list)\n",
        "\n",
        "class ResidualSublayer(nn.Module):\n",
        "\n",
        "    def __init__(self, sublayer, size, dropout=0.5):\n",
        "\n",
        "        super(ResidualSublayer, self).__init__()\n",
        "        self.sublayer = sublayer\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, *x):\n",
        "\n",
        "        return self.norm(x[0] + self.dropout(self.sublayer(*x)))\n",
        "\n",
        "\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_size=512, ff_dim=2048, head_num=8):\n",
        "\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.multi_head_attn = ResidualSublayer(\n",
        "            MultiHeadAttention(head_num, layer_size),\n",
        "            layer_size)\n",
        "        self.ff_network = ResidualSublayer(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(layer_size, ff_dim), \n",
        "                nn.ReLU(),\n",
        "                nn.Linear(ff_dim, layer_size)\n",
        "            ), layer_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.multi_head_attn(x,x,x)\n",
        "\n",
        "        x = self.ff_network(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_size=512, N=6):\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.norm = LayerNorm(layer_size)\n",
        "        self.layers = nn.ModuleList([TransformerEncoderLayer() for _ in range(N)])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_size=512, ff_dim=2048, head_num=8):\n",
        "\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.attention_1 = ResidualSublayer(\n",
        "            MultiHeadAttention(head_num, layer_size),\n",
        "            layer_size)\n",
        "        self.attention_2 = ResidualSublayer(\n",
        "            MultiHeadAttention(head_num, layer_size),\n",
        "            layer_size)\n",
        "        self.ff_network = ResidualSublayer(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(layer_size, ff_dim), \n",
        "                nn.ReLU(),\n",
        "                nn.Linear(ff_dim, layer_size)\n",
        "            ), layer_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "        \n",
        "        x = self.attention_1(x, x, x)\n",
        "\n",
        "        x = self.attention_2(x, memory, memory)\n",
        "\n",
        "        x = self.ff_network(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_size=512, N=6):\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([TransformerDecoderLayer() for _ in range(N)])\n",
        "\n",
        "    def forward(self, x, memory):\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory)\n",
        "        return x"
      ],
      "metadata": {
        "id": "K3J5oX2IxqNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder()"
      ],
      "metadata": {
        "id": "3Kzd-ZGJaZyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = encoder(torch.rand((10, 512)))"
      ],
      "metadata": {
        "id": "x7jtjp_rbai6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hevkf2gfcuoX",
        "outputId": "50824790-abce-4163-9411-eb495e64d594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder()(torch.rand(10, 100, 512), memory).shape"
      ],
      "metadata": {
        "id": "xfvXD5-iJ2Co",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4959d15-6ebe-4038-bd62-5449a7d08b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 100, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}